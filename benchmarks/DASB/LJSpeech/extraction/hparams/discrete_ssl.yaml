# ############################################################################
# Auido Tokenizer: WavLM
# Extraction: Librispeech 960h
# Authors: Jarod Duret 2024
# ############################################################################
# Seed needs to be set at top of yaml, before objects with parameters are made

seed: 1986
__set_seed: !apply:torch.manual_seed [!ref <seed>]
output_folder: !ref results/wavlm
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/extraction_log.txt

# Data files
data_folder: !PLACEHOLDER  # e.g., /path/to/LJSpeech
splits: ["train", "valid", "test"]
split_ratio: [90, 5, 5]
frozen_split_path: null
skip_prep: False
train_json: !ref <output_folder>/train.json
valid_json: !ref <output_folder>/valid.json
test_json: !ref <output_folder>/test.json

batch_size: 8
num_workers: 8
src_key: wav
id_key: id

# Dataloader options
dataloader_opts:
  batch_size: !ref <batch_size>
  shuffle: True
  num_workers: !ref <num_workers>

### Configuration for  discrete SSL model
# | SSL Model  | HF Encoder                             | K-Means Dataset | K-Means Size | SSL Layers           | Vocoder Model                            |
# |------------|----------------------------------------|-----------------|--------------|----------------------|------------------------------------------|
# | WavLM      | microsoft/wavlm-large                  | LibriSpeech960  | 1000         | 1, 3, 7, 12, 18, 23  | speechbrain/hifigan-wavlm-k1000-LibriTTS |
# | HuBERT     | facebook/hubert-large-ll60k            | LibriSpeech960  | 1000         | 1, 3, 7, 12, 18, 23  | WIP                                      |
# | Wav2Vec2   | facebook/wav2vec2-large-960h-lv60-self | LibriSpeech960  | 1000         | 1, 3, 7, 12, 18, 23  | WIP                                      |

# ssl_model_type: hubert, wavlm, wav2vec2
# ssl_hub: facebook/hubert-large-ll60k, microsoft/wavlm-large,  facebook/wav2vec2-large
ssl_model_type: WavLM
ssl_hub: microsoft/wavlm-large
ssl_folder: !ref <save_folder>/ssl_checkpoint
kmeans_cache_dir: !ref <save_folder>/kmeans_checkpoint
kmeans_dataset: LibriSpeech
vocoder_repo_id: speechbrain/hifigan-wavlm-k1000-LibriTTS
freeze_ssl: True
freeze_feature_extractor: True
vocab_size: 1000
save_embedding: False

### Config for Tokenizer
# Layer number should be among the supported layers for discrete SSL models(kmenas  model should be available for that layer)
num_codebooks: [1, 3, 7, 12, 18, 23]
deduplicate: [False, False, False, False, False, False]
bpe_tokenizer_path: [null, null, null, null, null, null]
sample_rate: 16000
encoder_dim: 1024

ssl_model: !apply:speechbrain.utils.hparams.choice
  value: !ref <ssl_model_type>
  choices:
    WavLM: !new:speechbrain.lobes.models.huggingface_transformers.wavlm.WavLM
      source: !ref <ssl_hub>
      output_norm: False
      freeze: !ref <freeze_ssl>
      freeze_feature_extractor: !ref <freeze_feature_extractor>
      output_all_hiddens: True
      save_path: !ref <ssl_folder>
    HuBERT: !new:speechbrain.lobes.models.huggingface_transformers.hubert.HuBERT
      source: !ref <ssl_hub>
      output_norm: False
      freeze: !ref <freeze_ssl>
      freeze_feature_extractor: !ref <freeze_feature_extractor>
      output_all_hiddens: True
      save_path: !ref <ssl_folder>
    Wav2Vec2: !new:speechbrain.lobes.models.huggingface_transformers.wav2vec2.Wav2Vec2
      source: !ref <ssl_hub>
      output_norm: False
      freeze: !ref <freeze_ssl>
      freeze_feature_extractor: !ref <freeze_feature_extractor>
      output_all_hiddens: True
      save_path: !ref <ssl_folder>

tokenizer: !new:utils.tokenizer_interface.DiscreteSSLTokenizer
  save_path: !ref <kmeans_cache_dir>
  ssl_model: !ref <ssl_model>
  vocoder_repo_id: !ref <vocoder_repo_id>
  kmeans_dataset: !ref <kmeans_dataset>
  num_clusters: !ref <vocab_size>

tokens_extractor: !new:utils.tokens.TokensExtractor
  tokenizer: !ref <tokenizer>
  sample_rate: !ref <sample_rate>
  src_key: !ref <src_key>
  id_key: !ref <id_key>
  dataloader_opts: !ref <dataloader_opts>
