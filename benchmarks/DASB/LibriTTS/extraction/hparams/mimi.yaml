# ############################################################################
# Auido Tokenizer: Speech Tokenizer
# Extraction: Librispeech 960h
# Authors: Jarod Duret 2024
# ############################################################################
# Seed needs to be set at top of yaml, before objects with parameters are made

seed: 1986
__set_seed: !apply:torch.manual_seed [!ref <seed>]
output_folder: !ref results/speech_tokenizer
save_folder: !ref <output_folder>/save
pretrained_model_save_folder: !ref <save_folder>
train_log: !ref <output_folder>/extraction_log.txt

# Data files
data_folder: !PLACEHOLDER  # e.g., /path/to/LibriSpeech
train_splits: ["train-clean-100"] #, "train-clean-360", "train-other-500"
dev_splits: ["dev-clean"]
test_splits: ["dev-clean", "test-clean", "test-other"]
skip_prep: False
train_json: !ref <output_folder>/train.json
valid_json: !ref <output_folder>/dev-clean.json
test_json: !ref <output_folder>/test.json


batch_size: 8
num_workers: 8
src_key: wav
id_key: id

# Dataloader options
dataloader_opts:
  batch_size: !ref <batch_size>
  shuffle: True
  num_workers: !ref <num_workers>

####################### Model parameters ###########################
# Tokenizer parameters
model_hub: kyutai/mimi
vocab_size: 1024
num_codebooks: 32
sample_rate: 24000
encoder_dim: 1024
freeze_embedding: False
save_embedding: False
skip_resample: False

tokenizer: !new:utils.tokenizer_interface.MimiTokenizer
  source: !ref <model_hub>
  save_path: !ref <pretrained_model_save_folder>
  num_codebooks: !ref <num_codebooks>


tokens_extractor: !new:utils.tokens.TokensExtractor
  tokenizer: !ref <tokenizer>
  sample_rate: !ref <sample_rate>
  src_key: !ref <src_key>
  id_key: !ref <id_key>
  dataloader_opts: !ref <dataloader_opts>
